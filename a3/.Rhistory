# Filter for anomolies first bassically from last as1 where row and col that has a Z-score > 3 or < -3
df_cleaned <- df_zscores %>%
filter(!apply(select(., ends_with("_z")), 1, function(x) any(abs(x) > 3)))
# also dont need Z-score col
df_cleaned <- df_cleaned %>% select(-ends_with("_z"))
# Identify week start date for each row
df_cleaned$weekStart <- floor_date(
df_cleaned$DateTime,
unit = "week",
week_start = 1
)
# Slice dataset into complete weeks
df_cleaned <- df_cleaned %>%
group_by(weekStart) %>%
filter(n_distinct(Date) == 7) %>%
ungroup()
# Initialize smoothened_Global_intensity column
df_cleaned$smoothened_Global_intensity <- NA
window_size <- 7
# Compute moving average
for (week in unique(df_cleaned$weekStart)) {
# Rows for the current week
rows <- df_cleaned$weekStart == week
smooth_values <- rollmean(df_cleaned$Global_intensity[rows],
k = window_size,
fill = NA,
align = "center")
df_cleaned$smoothened_Global_intensity[rows] <- smooth_values
}
# Each smoothened week is a dataframe
smoothened_weeks <- split(df_cleaned, df_cleaned$weekStart)
# Rename the weeks from: smoothened_week_1 to smoothened_week_n
names(smoothened_weeks) <- paste0(
"smoothened_week_",
seq_along(smoothened_weeks))
#------------------------- Average Smoothened Week ------------------------
# Time series from the average Smoothed week
average_smoothened_week <- df_cleaned %>%
group_by(Time = format(DateTime, "%H:%M:%S")) %>%
summarise(average_intensity = mean(smoothened_Global_intensity, na.rm = TRUE))
print(average_smoothened_week)
#----------------------- Most & Least Anomalous Weeks ---------------------
# Function to compute Mean Absolute Deviation (MAD)
mad_score <- function(actual, predicted) {
mean(abs(actual - predicted), na.rm = TRUE)
}
# Store into a data point
anomaly_scores <- data.frame(Week = names(smoothened_weeks), Score = NA)
# Compute the score for each week
for (i in seq_along(smoothened_weeks)) {
week_data <- smoothened_weeks[[i]]
# Merged both with similar 'Time'
merged_data <- merge(week_data, average_smoothened_week, by="Time")
# than compute mad score
anomaly_scores$Score[i] <- mad_score(merged_data$smoothened_Global_intensity, merged_data$average_intensity)
}
# Get least and most anomalous_week
most_anomalous_week <- anomaly_scores[which.max(anomaly_scores$Score), "Week"]
least_anomalous_week <- anomaly_scores[which.min(anomaly_scores$Score), "Week"]
# Results
print(anomaly_scores)
cat("Most Anomalous Week:", most_anomalous_week, "\n")
cat("Least Anomalous Week:", least_anomalous_week, "\n")
#----------------------- Visualization ---------------------
# Convert to POSIXct
average_smoothened_week$Time <- as.POSIXct(average_smoothened_week$Time, format="%H:%M:%S", tz="UTC")
most_anomalous_data$Time <- as.POSIXct(most_anomalous_data$Time, format="%H:%M:%S", tz="UTC")
library(zoo)
library(ggplot2)
library(reshape2)
#--------------------------------------------------------------------------
#                                 Part 1
#--------------------------------------------------------------------------
#--------------------------------------------------------------------------
# (i) Linear Interpolation
#--------------------------------------------------------------------------
Group_Assignment_Dataset <- read.csv("Group_Assignment_Dataset.txt")
numeric_cols <- names(Group_Assignment_Dataset)[sapply(Group_Assignment_Dataset, is.numeric)]
df_interpolated <- Group_Assignment_Dataset
cat("Number of missing values per column before interpolation:\n")
print(colSums(is.na(df_interpolated)))
for (col in numeric_cols) {
df_interpolated[[col]] <- na.approx(df_interpolated[[col]], rule = 2)
}
cat("Number of missing values per column after interpolation:\n")
print(colSums(is.na(df_interpolated)))
#--------------------------------------------------------------------------
# (ii) Point anomalies
#--------------------------------------------------------------------------
# Calculate Z-scores
df_zscores <- df_interpolated
for (col in numeric_cols) {
col_mean <- mean(df_interpolated[[col]])
col_stdev <- sd(df_interpolated[[col]])
col_zscore <- (df_interpolated[[col]] - col_mean) / col_stdev
# Store results in new columns
new_col <- paste0(col, "_z")
df_zscores[[new_col]] <- col_zscore
}
# Track anomalies in a data frame
df_anomalies <- data.frame(
feature = character(),
num_anomalies = numeric(),
total_points = numeric(),
percent_anomalies = numeric()
)
overall_anomalies <- 0
overall_points <- 0
for (col in numeric_cols) {
# Access results from z-scores
z_col <- paste0(col, "_z")
z_vals <- df_zscores[[z_col]]
is_anomaly <- abs(z_vals) > 3 # Boolean value tracks if point is anomaly
num_anomalies <- sum(is_anomaly)
total_points  <- length(z_vals)
# Anomalies in the column = # anomalies / # points * 100
percent_anomalies <- (num_anomalies / total_points) * 100
# Store results
df_anomalies <- rbind(
df_anomalies,
data.frame(
feature = col,
num_anomalies = num_anomalies,
total_points = total_points,
percent_anomalies = percent_anomalies
)
)
# Aggregate total results
overall_anomalies <- overall_anomalies + num_anomalies
overall_points    <- overall_points + total_points
}
overall_percent <- (overall_anomalies / overall_points) * 100
cat("\nAnomaly summary per feature:\n")
print(df_anomalies)
cat("\nOverall anomaly summary:\n")
cat(round(overall_percent, 2), "%\n")
#--------------------------------------------------------------------------
# (iii) Extracting data for Week 5
#--------------------------------------------------------------------------
df_zscores$DateTime <- as.POSIXct(
paste(df_zscores$Date, df_zscores$Time),
format = "%d/%m/%Y %H:%M:%S"
)
start_date <- as.POSIXct("29/1/2007 00:00:00", format = "%d/%m/%Y %H:%M:%S")
end_date   <- as.POSIXct("4/2/2007 00:00:00",  format = "%d/%m/%Y %H:%M:%S")
df_Week5 <- subset(df_zscores, DateTime >= start_date & DateTime <= end_date)
print(start_date)
#--------------------------------------------------------------------------
# Part 2
#--------------------------------------------------------------------------
vars <- c("Global_active_power", "Global_reactive_power", "Voltage", "Global_intensity", "Sub_metering_1", "Sub_metering_2", "Sub_metering_3")
c_matrix <- cor(df_Week5[, vars], method = "pearson")
#print the correlation matrix
c_matrix
# Hepler funct to reorder values
reorder_cormat <- function(cormat) {
dd <- as.dist((1 - cormat) / 2)
hc <- hclust(dd)
cormat <- cormat[hc$order, hc$order]
return(cormat)
}
# Funct to get rid of duplicate
get_upper_tri <- function(cormat) {
cormat[lower.tri(cormat)] <- NA
return(cormat)
}
c_matrix <- reorder_cormat(c_matrix)
upper_tri <- get_upper_tri(c_matrix)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
#heatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
geom_tile(color = "white") +
geom_text(aes(label = round(value, 2)), color = "black", size = 4) +
scale_fill_gradient2(low = "#50e991", high = "#e60049", mid = "white",
midpoint = 0, limit = c(-1, 1), space = "Lab",
name="Pearson") +
theme_minimal() +
theme(
axis.title.x = element_blank(),
axis.title.y = element_blank(),
panel.grid.major = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
axis.ticks = element_blank(),
legend.justification = c(1, 0),
legend.position = c(0.6, 0.7),
legend.direction = "horizontal"
) +
guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
title.position = "top", title.hjust = 0.5))
# Print heatmap
print(ggheatmap)
#--------------------------------------------------------------------------
#                                 Part 3
#--------------------------------------------------------------------------
Group_Assignment_Dataset$DateTime <- as.POSIXct(paste(Group_Assignment_Dataset$Date
,Group_Assignment_Dataset$Time),
format="%d/%m/%Y %H:%M:%S")
# adding hour and day rows to the data set
# doing this because it is easier to filter the data into different
# categories
Group_Assignment_Dataset$Day <- weekdays(Group_Assignment_Dataset$DateTime)
#Group_Assignment_Dataset$Hour <- as.POSIXct(Group_Assignment_Dataset$DateTime, format = "%H:%M")
# subsetting the data into day time hours and night time hours
# using the hours specified in the assignment description
day_time_window <- Group_Assignment_Dataset[Group_Assignment_Dataset$Time >= "07:30:00"
& Group_Assignment_Dataset$Time <= "17:00:00",]
night_time_window <- Group_Assignment_Dataset[(Group_Assignment_Dataset$Time > "17:00:00"
& Group_Assignment_Dataset$Time <= "23:00:00"), ]
# subsetting the data based on weekday vs weekend and time window
# because the assignment made a distinction when talking about weekday and weekend
# NOTE: I'm not actually sure if we're supposed to make a distinction between
# weekdays and weekends
weekdays_data_day <- day_time_window[day_time_window$Day
%in% c("Monday", "Tuesday", "Wednesday",
"Thursday", "Friday"), ]
weekends_data_day <- day_time_window[day_time_window$Day
%in% c("Saturday","Sunday"), ]
weekdays_data_night <- night_time_window[night_time_window$Day
%in% c("Monday", "Tuesday", "Wednesday",
"Thursday", "Friday"), ]
weekends_data_night <- night_time_window[night_time_window$Day
%in% c("Saturday","Sunday"),]
# computing the averages, each time(Hour) with the same value, is grouped together
weekdays_avg_day <- aggregate(Global_intensity ~ Time, data = weekdays_data_day, FUN = mean)
weekends_avg_day <- aggregate(Global_intensity ~ Time, data = weekends_data_day, FUN = mean)
weekdays_avg_night <- aggregate(Global_intensity ~ Time, data = weekdays_data_night, FUN = mean)
weekends_avg_night <- aggregate(Global_intensity ~ Time, data = weekends_data_night, FUN = mean)
# fitting the models
# the time has to be converted to seconds cause it's not a numeric type on its own
# The seconds returned is the seconds after epoch (Jan 1st, 1970)
fit1 <- lm(Global_intensity ~ as.numeric(as.POSIXct(Time, format="%H:%M:%S")), data = weekdays_avg_day)
fit2 <- lm(Global_intensity ~ as.numeric(as.POSIXct(Time, format="%H:%M:%S")), data = weekends_avg_day)
fit3 <- lm(Global_intensity ~ as.numeric(as.POSIXct(Time, format="%H:%M:%S")), data = weekdays_avg_night)
fit4 <- lm(Global_intensity ~ as.numeric(as.POSIXct(Time, format="%H:%M:%S")), data = weekdays_avg_night)
polyfit1 = lm(Global_intensity ~ poly(as.numeric(as.POSIXct(Time, format="%H:%M:%S")), 2, raw=TRUE), data = weekdays_avg_day)
polyfit2 = lm(Global_intensity ~ poly(as.numeric(as.POSIXct(Time, format="%H:%M:%S")), 2, raw=TRUE), data = weekends_avg_day)
polyfit3 = lm(Global_intensity ~ poly(as.numeric(as.POSIXct(Time, format="%H:%M:%S")), 2, raw=TRUE), data = weekdays_avg_night)
polyfit4 = lm(Global_intensity ~ poly(as.numeric(as.POSIXct(Time, format="%H:%M:%S")), 2, raw=TRUE), data = weekends_avg_night)
ggplot() +
geom_smooth(data = weekdays_avg_day,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekday Day"),method = "lm",
formula = y ~ x, se = FALSE) +
geom_smooth(data = weekdays_avg_day,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekday Day"),method = "lm",
formula = y ~ poly(x,2), se = FALSE, linetype = "dashed") +
geom_point(data = weekdays_avg_day,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekday Day"), alpha = 0.2) +
geom_smooth(data = weekends_avg_day,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekend Day"),method = "lm",
formula = y ~ poly(x,2), se = FALSE, linetype = "dashed") +
geom_smooth(data = weekends_avg_day,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekend Day"),method = "lm",
formula = y ~ x, se = FALSE) +
geom_point(data = weekends_avg_day,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekend Day"), alpha = 0.2) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)  # Rotate x-axis labels by 45 degrees
) +
scale_color_manual(name = "Legend",
values = c("Weekday Day" = "red",
"Weekend Day" = "blue",
"Weekday Night" = "green",
"Weekend Night" = "yellow")) +
labs(title = "Daytime Global intensity vs Time",x = "Time", y = "Global intensity")
ggplot() +
geom_smooth(data = weekdays_avg_night,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekday Night"),method = "lm",
formula = y ~ x, se = FALSE) +
geom_point(data = weekdays_avg_night,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekday Night"), alpha = 0.2) +
geom_smooth(data = weekends_avg_night,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekend Night"),method = "lm",
formula = y ~ x, se = FALSE) +
geom_point(data = weekends_avg_night,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekend Night"), alpha = 0.2) +
geom_smooth(data = weekdays_avg_night,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekday Night"),method = "lm",
formula = y ~ poly(x,2), se = FALSE, linetype = "dashed") +
geom_smooth(data = weekends_avg_night,
aes(x = as.POSIXct(Time, format="%H:%M:%S"),
y = Global_intensity, color = "Weekend Night"),method = "lm",
formula = y ~ poly(x,2), se = FALSE, linetype = "dashed") +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)  # Rotate x-axis labels by 45 degrees
) +
scale_color_manual(name = "Legend",
values = c("Weekday Day" = "red",
"Weekend Day" = "blue",
"Weekday Night" = "green",
"Weekend Night" = "yellow")) +
labs(title = "Nightime Global intensity vs Time",x = "Time", y = "Global intensity")
#--------------------------------------------------------------------------
#                                   Q2
#--------------------------------------------------------------------------
library(dplyr)
library(lubridate)
library(zoo)
library(ggplot2)
#---------------------------- Moving Average ------------------------------
# Filter for anomolies first bassically from last as1 where row and col that has a Z-score > 3 or < -3
df_cleaned <- df_zscores %>%
filter(!apply(select(., ends_with("_z")), 1, function(x) any(abs(x) > 3)))
# also dont need Z-score col
df_cleaned <- df_cleaned %>% select(-ends_with("_z"))
# Identify week start date for each row
df_cleaned$weekStart <- floor_date(
df_cleaned$DateTime,
unit = "week",
week_start = 1
)
# Slice dataset into complete weeks
df_cleaned <- df_cleaned %>%
group_by(weekStart) %>%
filter(n_distinct(Date) == 7) %>%
ungroup()
# Initialize smoothened_Global_intensity column
df_cleaned$smoothened_Global_intensity <- NA
window_size <- 7
# Compute moving average
for (week in unique(df_cleaned$weekStart)) {
# Rows for the current week
rows <- df_cleaned$weekStart == week
smooth_values <- rollmean(df_cleaned$Global_intensity[rows],
k = window_size,
fill = NA,
align = "center")
df_cleaned$smoothened_Global_intensity[rows] <- smooth_values
}
# Each smoothened week is a dataframe
smoothened_weeks <- split(df_cleaned, df_cleaned$weekStart)
# Rename the weeks from: smoothened_week_1 to smoothened_week_n
names(smoothened_weeks) <- paste0(
"smoothened_week_",
seq_along(smoothened_weeks))
#------------------------- Average Smoothened Week ------------------------
# Time series from the average Smoothed week
average_smoothened_week <- df_cleaned %>%
group_by(Time = format(DateTime, "%H:%M:%S")) %>%
summarise(average_intensity = mean(smoothened_Global_intensity, na.rm = TRUE))
print(average_smoothened_week)
#----------------------- Most & Least Anomalous Weeks ---------------------
# Function to compute Mean Absolute Deviation (MAD)
mad_score <- function(actual, predicted) {
mean(abs(actual - predicted), na.rm = TRUE)
}
# Store into a data point
anomaly_scores <- data.frame(Week = names(smoothened_weeks), Score = NA)
# Compute the score for each week
for (i in seq_along(smoothened_weeks)) {
week_data <- smoothened_weeks[[i]]
# Merged both with similar 'Time'
merged_data <- merge(week_data, average_smoothened_week, by="Time")
# than compute mad score
anomaly_scores$Score[i] <- mad_score(merged_data$smoothened_Global_intensity, merged_data$average_intensity)
}
# Get least and most anomalous_week
most_anomalous_week <- anomaly_scores[which.max(anomaly_scores$Score), "Week"]
least_anomalous_week <- anomaly_scores[which.min(anomaly_scores$Score), "Week"]
# Results
print(anomaly_scores)
cat("Most Anomalous Week:", most_anomalous_week, "\n")
cat("Least Anomalous Week:", least_anomalous_week, "\n")
library(depmixS4)
library(dplyr)
library(lubridate)
library(zoo)
library(ggplot2)
#-------------------------------------------------------
#  Question 1
#-------------------------------------------------------
# ------ Choose a time window & filter ------ #
chosen_day_of_week <- 2  # 1=Monday, 2=Tuesday, 3=Wednesday, ...
start_hour <- 12
end_hour   <- 18
# Filter data for that day-of-week + hour range
df_window <- df_cleaned %>%
filter(
wday(DateTime) == chosen_day_of_week,
hour(DateTime) >= start_hour & hour(DateTime) < end_hour
)
# Get number of rows per week based on week start date
counts_by_week <- df_window %>%
group_by(weekStart) %>%
summarise(nrows = n())
# Extract number of rows for each week into a vector
ntimes_vec <- counts_by_week$nrows
# ------ HMM's ------ #
# State range for models
state_range <- 4:16
# Data frame to store results
results <- data.frame(
numStates = integer(),
logLik    = numeric(),
BIC       = numeric(),
stringsAsFactors = FALSE
)
for (ns in state_range) {
# Build the HMM model
model <- depmix(
response = Global_active_power ~ 1,
data     = df_window,
nstates  = ns,
ntimes   = ntimes_vec
)
# Fit the model
fit_model <- fit(model, verbose = FALSE)
# Get log-likelihood and BIC values
ll  <- logLik(fit_model)
bic <- BIC(fit_model)
# Save
results <- rbind(
results,
data.frame(
numStates = ns,
logLik = as.numeric(ll),
BIC = bic
)
)
}
library(depmixS4)
library(dplyr)
library(lubridate)
library(zoo)
library(ggplot2)
#-------------------------------------------------------
#  Question 1
#-------------------------------------------------------
# ------ Choose a time window & filter ------ #
chosen_day_of_week <- 2  # 1=Monday, 2=Tuesday, 3=Wednesday, ...
start_hour <- 12
end_hour   <- 18
# Filter data for that day-of-week + hour range
df_window <- df_cleaned %>%
filter(
wday(DateTime) == chosen_day_of_week,
hour(DateTime) >= start_hour & hour(DateTime) < end_hour
)
# Get number of rows per week based on week start date
counts_by_week <- df_window %>%
group_by(weekStart) %>%
summarise(nrows = n())
# Extract number of rows for each week into a vector
ntimes_vec <- counts_by_week$nrows
# ------ HMM's ------ #
# State range for models
state_range <- 4:16
# Data frame to store results
results <- data.frame(
numStates = integer(),
logLik    = numeric(),
BIC       = numeric(),
stringsAsFactors = FALSE
)
for (ns in state_range) {
# Build the HMM model
model <- depmix(
response = Global_active_power ~ 1,
data     = df_window,
nstates  = ns,
ntimes   = ntimes_vec
)
# Fit the model
fit_model <- fit(model, verbose = FALSE)
# Get log-likelihood and BIC values
ll  <- logLik(fit_model)
bic <- BIC(fit_model)
# Save
results <- rbind(
results,
data.frame(
numStates = ns,
logLik = as.numeric(ll),
BIC = bic
)
)
}
# ------ Inspect results ------ #
print(results)
summary(fit_model)
# Plot -> numStates vs. LL
plot(results$numStates, results$logLik,
type = "b", pch = 19,
xlab = "Number of States",
ylab = "Log-Likelihood",
main = "States vs. log-likelihood"
)
# Plot -> numStates vs. BIC
plot(results$numStates, results$BIC,
type = "b", pch = 19, col = "red",
xlab = "Number of States",
ylab = "BIC",
main = "States vs. BIC"
)
head(df_clean)
head(df_window)
df_round$Global_active_power <- round(df_round$Global_active_power * 2) / 2
df_round <- df_window
df_round$Global_active_power <- round(df_round$Global_active_power * 2) / 2
head(df_window)
head(df_round)
